{"cells":[{"source":["from pyspark.sql.types import *\n","import pyspark.sql.functions as F\n","from elasticsearch import Elasticsearch, helpers\n","import requests\n","import numpy as np\n","from pyspark.sql.window import Window\n","from pyspark.sql.functions import when, col, lag, date_format, countDistinct, from_utc_timestamp, row_number, lit, month, hour\n","from pyspark.sql.functions import randn, rand\n","import time\n","import datetime\n","from pyspark.ml.classification import LogisticRegression\n","from pyspark.sql import types\n","import numpy as np\n","import pyspark.sql.functions\n","from sklearn.metrics import confusion_matrix\n","from pyspark.ml.feature import VectorAssembler\n","from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n","from pyspark.ml import Pipeline\n","from pyspark.ml.feature import VectorIndexer\n","from pyspark.ml.evaluation import BinaryClassificationEvaluator\n","from pyspark.streaming import *\n","from pyspark.sql.functions import *\n","import pickle\n","import pyspark.sql.functions as F"],"cell_type":"code","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0e457608-1440-4ee7-a37c-ad2e910e5db1"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["schema = types.StructType([\n","    types.StructField('_id',                 types.StructType([\n","                                                types.StructField('$oid', types.StringType(), True),\n","                                                ]), True),\n","    types.StructField('actualDelay',         types.LongType(),    True),\n","    types.StructField('angle',               types.DoubleType(),  True),\n","    types.StructField('anomaly',             types.BooleanType(), True),\n","    types.StructField('areaId',              types.LongType(),    True),\n","    types.StructField('areaId1',             types.LongType(),    True),\n","    types.StructField('areaId2',             types.LongType(),    True),\n","    types.StructField('areaId3',             types.LongType(),    True),\n","    types.StructField('atStop',              types.BooleanType(), True),\n","    types.StructField('busStop',             types.LongType(),    True),\n","    types.StructField('calendar',            types.StructType([\n","                                                types.StructField('$numberLong', types.StringType(), True),\n","                                                ]), True),\n","    types.StructField('congestion',          types.BooleanType(), True),\n","    types.StructField('currentHour',         types.LongType(),    True),\n","    types.StructField('dateType',            types.LongType(),    True),\n","    types.StructField('dateTypeEnum',        types.StringType(),  True),\n","    types.StructField('delay',               types.LongType(),    True),\n","    types.StructField('direction',           types.LongType(),    True),\n","    types.StructField('distanceCovered',     types.DoubleType(),  True),\n","    types.StructField('ellapsedTime',        types.LongType(),    True),\n","    types.StructField('filteredActualDelay', types.LongType(),    True),\n","    types.StructField('gridID',              types.StringType(),  True),\n","    types.StructField('journeyPatternId',    types.StringType(),  True),\n","    types.StructField('justLeftStop',        types.BooleanType(), True),\n","    types.StructField('justStopped',         types.BooleanType(), True),\n","    types.StructField('latitude',            types.DoubleType(),  True),\n","    types.StructField('lineId',              types.StringType(),  True),\n","    types.StructField('loc',                 types.StructType([\n","                                                types.StructField('coordinates', types.ArrayType(types.DoubleType(), True), True),\n","                                                types.StructField('type', types.StringType(), True),\n","                                                ]), True),\n","    types.StructField('longitude',           types.DoubleType(), True),\n","    types.StructField('poiId',               types.LongType(),   True),\n","    types.StructField('poiId2',              types.LongType(),   True),\n","    types.StructField('probability',         types.DoubleType(), True),\n","    types.StructField('systemTimestamp',     types.DoubleType(), True),\n","    types.StructField('timestamp',           types.StructType([\n","                                                types.StructField('$numberLong', types.StringType(), True),\n","                                                ]), True),\n","    types.StructField('vehicleId',           types.LongType(), True),\n","    types.StructField('vehicleSpeed',        types.LongType(), True),\n","])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3c989b16-1028-4f18-a7bc-067bbb1afc40"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"source":["### Read static buses data"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["# Extraction of the dublin bus data from Spark's File storage with the known schema\n","df = spark.read.json('/mnt/dacoursedatabricksstg/dacoursedatabricksdata/busFile',schema)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Read static buses data","showTitle":true,"inputWidgets":{},"nuid":"a272026c-eaf1-4ba1-8062-9daa59c2abfb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"source":["### Read external data that uploaded from app - batch"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_external_data = spark.read.json('/FileStore/Ron_Eden/FromWeb/Stream',schema)"]},{"source":["### Read external data that uploaded from app - stream"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_external_data = spark.readStream.json('/FileStore/Ron_Eden/FromWeb/Stream',schema)"]},{"source":["### Data preprocess - Task 1"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["# flatten_df = flats all the structs columns so it will be easier to read/save\n","def flatten_df(nested_df):\n","    flat_cols = [c[0] for c in nested_df.dtypes if c[1][:6] != 'struct']\n","    nested_cols = [c[0] for c in nested_df.dtypes if c[1][:6] == 'struct']\n","\n","    flat_df = nested_df.select(flat_cols +\n","                               [F.col(nc+'.'+c).alias(nc+'_'+c)\n","                                for nc in nested_cols\n","                                for c in nested_df.select(nc+'.*').columns])\n","    return flat_df\n","\n","def preprocess_data(df):\n","  ################################# Prepare the data for streanimg (no trip_id) #################################\n","  flat_df = flatten_df(df)\n","  # Rename columns\n","  flat_df = flat_df.withColumnRenamed('timestamp_$numberLong', 'timestamp_numberLong')\n","  flat_df = flat_df.withColumnRenamed('calendar_$numberLong', 'calendar_numberLong')\n","  flat_df = flat_df.withColumnRenamed('_id_$oid', '_idRecord')\n","  flat_df = flat_df.withColumnRenamed('dateTypeEnum', 'isWeekend')\n","  # Convert timestamp columns to timestamp time from Long int\n","  flat_df = flat_df.withColumn('timestamp', F.to_timestamp(F.from_unixtime(flat_df.timestamp_numberLong / (1000))))\n","  flat_df = flat_df.withColumn('calendar', F.to_date(F.from_unixtime(flat_df.calendar_numberLong / (1000000))))\n","  flat_df = flat_df.drop(\"timestamp_numberLong\", \"calendar_numberLong\", \"loc_type\", \"calendar\")\n","  flat_df = flat_df.withColumn(\"date\", F.to_date(F.col(\"timestamp\"))).withColumn(\"month\", month(\"timestamp\")).withColumn(\"hour\", hour(\"timestamp\").cast(\"bigint\"))\n","  flat_df = flat_df.withColumn('DayInWeek', date_format('date', 'E'))\n","  flat_df = flat_df.withColumn(\n","      'season',\n","      F.when((F.col(\"month\")).between(6, 8),1 )\\\n","      .when((F.col(\"month\")).between(3,5), 2)\\\n","      .when((F.col(\"month\")).between(9,11), 3)\\\n","      .otherwise(0)\n","  )\n","  flat_df = flat_df.withColumn(\n","      'rushHours',\n","      F.when((F.col(\"hour\")).between(7, 9) | (F.col(\"hour\")).between(16, 18), 1)\\\n","      .otherwise(0)\n","  )\n","  udf_loc = F.udf(lambda x,y: [x, y], \"array<float>\")\n","  flat_df = flat_df.withColumn('geoCoordinate', udf_loc(flat_df.latitude,flat_df.longitude))\n","  ################################# Take relevant columns #################################\n","  flat_df = flat_df[['_idRecord', 'rushHours', 'hour', 'date', 'season', 'longitude','latitude', 'areaId1', 'vehicleSpeed', 'timestamp', 'congestion', 'delay', 'lineId', 'justStopped',  'justLeftStop','atStop', 'ellapsedTime', 'journeyPatternId', 'DayInWeek' ]]\n","  ################################# Convert congestion type to numeric #################################\n","  union_df = flat_df.withColumn(\n","      'congestion',\n","      F.when((F.col(\"congestion\")) == 'true', 1 )\\\n","      .otherwise(0)\n","  )\n","  geo_udf = udf(lambda x,y: [y, x], \"array<float>\")\n","  union_df = union_df.withColumn('geo', geo_udf(union_df.latitude, union_df.longitude))\n","  return union_df\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Data preprocess - Task 1","showTitle":true,"inputWidgets":{},"nuid":"ad2b0c04-affa-4277-b1c7-4a8d9a91d3be"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"source":["### Connection to Elastic"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["ES_HOST = '10.0.0.19'\n","es = Elasticsearch([{'host' :ES_HOST }], timeout=60000)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Connection to Elastic","showTitle":true,"inputWidgets":{},"nuid":"187fa77f-6455-40a3-9598-0cfbb00a7695"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"source":["### Write steam data to Elastic Search"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["def write_stream(df, index: str, settings_for_elastic, checkpoints_location):\n","  es.indices.create(index=index, ignore=400, body=settings_for_elastic)\n","  df.writeStream \\\n","      .outputMode(\"append\") \\\n","      .queryName(f\"{index}_to_es\") \\\n","      .format(\"org.elasticsearch.spark.sql\") \\\n","      .option(\"es.nodes.wan.only\",\"true\") \\\n","      .option(\"checkpointLocation\", checkpoints_location) \\\n","      .option(\"es.resource\", index) \\\n","      .option(\"es.nodes\", ES_HOST) \\\n","      .option(\"es.port\",\"9200\") \\\n","      .start()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Write steam data to Elastic Search","showTitle":true,"inputWidgets":{},"nuid":"7961cfb8-b715-4f5a-8f46-ef63c900bab5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"source":["### Settings for elastic"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["SETTINGS = {\n","    \"settings\": {\n","        \"number_of_shards\": 1,\n","        \"number_of_replicas\": 0,\n","        \"refresh_interval\" : -1\n","    },\n","     \"mappings\": {\n","      \"properties\": {\n","          \"actualDelay\" : { \"type\": \"long\" },\n","          \"areaId1\" : { \"type\": \"long\" },\n","          \"busStop\" : { \"type\": \"long\" },\n","          \"congestion\" : { \"type\": \"boolean\" },\n","          \"journeyPatternId\" : { \"type\": \"keyword\" },\n","          \"lineId\" : { \"type\": \"keyword\" },\n","          \"timestamp\" : { \"type\": \"date\"},\n","          \"vehicleId\" : { \"type\": \"long\" },\n","          \"vehicleSpeed\" : { \"type\": \"long\" },\n","          \"_idRecord\" : { \"type\": \"keyword\" },\n","          \"date\" : {\"type\":\"date\"},\n","          \"month\" : {\"type\": \"long\"}, \n","          \"hour\" : {\"type\": \"long\"},\n","          \"season\" : {\"type\": \"long\"},\n","          \"rain\" : {\"type\": \"long\"},\n","          \"temp\" : {\"type\": \"long\"},\n","          \"sun\" : {\"type\": \"long\"},\n","          \"wdsp\" : {\"type\": \"long\"},\n","          \"event\" : {\"type\": \"boolean\"},\n","          \"rushHours\" : {\"type\": \"long\"},\n","          \"geo\": {\"type\": \"geo_point\"},\n","    }\n","     }\n","}\n","\n","SETTINGS2 = {\n","    \"settings\": {\n","        \"number_of_shards\": 1,\n","        \"number_of_replicas\": 0,\n","        \"refresh_interval\" : -1\n","    },\n","     \"mappings\": {\n","      \"properties\": {\n","          \"date\" : {\"type\":\"date\"},\n","          \"hour\" : {\"type\": \"long\"},\n","          \"lineId_index\" : {\"type\": \"long\"},\n","          \"prediction\" : {\"type\": \"long\"},\n","          \"areaId1\" : { \"type\": \"long\" },\n","          \"journeyPatternId\" : { \"type\": \"keyword\" },\n","          \n","    }\n","     }\n","}"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Settings for elastic","showTitle":true,"inputWidgets":{},"nuid":"e3f520e7-968b-4b46-af16-357a67b02db6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"source":["### Uncertain data - Task 2"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["def filter_uncertain_data(union_df):\n","  ## Filter uncertain data\n","  union_df_filter_uncertain = union_df.withColumn(\n","      'vehicleSpeed',\n","      F.when((F.col(\"congestion\") == 0) & (F.col(\"atStop\") == False) & (F.col(\"justLeftStop\") == False) &(F.col(\"justStopped\") == False) &(F.col(\"vehicleSpeed\") == 0), np.nan)\n","      .otherwise(F.col(\"vehicleSpeed\"))\n","  )\n","\n","  union_df_filter_uncertain = union_df_filter_uncertain.withColumn('vehicleSpeed',\n","      F.when((F.col(\"vehicleSpeed\")>= 0) & (F.col(\"vehicleSpeed\")<= 180)  , F.col(\"vehicleSpeed\"))\n","      .otherwise(np.nan)\n","  )\n","  return union_df_filter_uncertain"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Uncertain data - Task 2","showTitle":true,"inputWidgets":{},"nuid":"3d46e3ae-7441-47e9-aa88-35111bb18411"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"source":["### Preprocess for prediction - Task 2"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["def prepare_columns_for_regression(df,categoricalCols, continuousCols,labelCol):\n","    for c, c_file in categoricalCols.items():\n","      df_hash = spark.read.format(\"csv\").option(\"header\", \"true\").option('sep','|').load(c_file, inferSchema=\"true\")\n","      df_hash = df_hash.withColumnRenamed('index', f'{c}_index')\n","      df_hash = df_hash.withColumnRenamed(c, f'{c}_1')\n","      df = df.join(df_hash.hint(\"broadcast\"), df_hash[f'{c}_1'] == df[c], how='inner')\n","      df = df.drop(f'{c}_1')\n","    index_cols = ['areaId1_index', 'lineId_index', 'journeyPatternId_index']\n","    assembler = VectorAssembler(inputCols= index_cols + continuousCols, outputCol=\"features\")\n","    pipeline = Pipeline(stages=[assembler])\n","    model=pipeline.fit(df)\n","    data = model.transform(df)\n","    data = data.withColumn('label',col(labelCol))\n","    return data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Preprocess for prediction - Task 2","showTitle":true,"inputWidgets":{},"nuid":"c0172ae2-da1c-44a5-9289-b97ca23d414f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"source":["### Train the model (Logistic Regression) - Task 2"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["def train(data, model_name):\n","  logr = LogisticRegression(featuresCol='features', labelCol='label')\n","  model = logr.fit(data)\n","  model.save(model_name)\n","  return model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Train the model (Logistic Regression) - Task 2","showTitle":true,"inputWidgets":{},"nuid":"c2f7d559-6474-4054-abf2-3cab18e6617a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"source":["### Evaluation - Task 2"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["def evaluation(predictions, classes, type):\n","  y_true = predictions.select(\"label\")\n","  y_true = y_true.toPandas()\n","  y_pred = predictions.select(\"prediction\")\n","  y_pred = y_pred.toPandas()\n","  y_pred['prediction'] = y_pred['prediction'].astype('int32')\n","  cm2 = confusion_matrix(y_true, y_pred,labels=classes)\n","  accuracy=(cm2[0][0]+cm2[1][1])/cm2.sum()\n","  precision=(cm2[0][0])/(cm2[0][0]+cm2[1][0])\n","  recall=(cm2[0][0])/(cm2[0][0]+cm2[0][1])\n","  print(f\"LogisticRegression {type}: accuracy,precision,recall\",accuracy,precision,recall)\n","  evaluator=BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\",labelCol=\"label\")\n","  # Get the ROC\n","  predictions.select(\"label\",\"rawPrediction\",\"prediction\",\"probability\")\n","  print(\"The area under ROC for test set is {}\".format(evaluator.evaluate(predictions)))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Evaluation - Task 2","showTitle":true,"inputWidgets":{},"nuid":"da6a1d8f-88b2-49d2-8a2f-15106ceb77e9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"source":["### Integration of weather data - Task 3"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["def integrate_weather_data(df):\n","  # df is stream\n","  df_weather = spark.read.csv(\"/FileStore/tables/weather_ron_eden_updated.csv\", header = True)\n","  df_weather = df_weather.withColumnRenamed('date', 'timestamp')\n","  df_weather = df_weather.withColumn(\"date\", F.split(col(\"timestamp\"), \" \").getItem(0)).withColumn(\"hour\", F.split(col(\"timestamp\"), \" \").getItem(1))\n","  df_weather = df_weather.withColumn('date', F.to_date(df_weather.date, 'dd/MM/yyyy'))\n","  df_weather = df_weather.withColumn('hour', df_weather.hour[0:2].cast(IntegerType()))\n","  df_weather = df_weather.withColumn(\"temp\", df_weather[\"temp\"].cast(DoubleType()))\n","  df_weather = df_weather.withColumn(\"rain\", df_weather[\"rain\"].cast(DoubleType()))\n","  df_weather = df_weather.withColumn(\"sun\", df_weather[\"sun\"].cast(DoubleType()))\n","  df_weather = df_weather.withColumn(\"wdsp\", df_weather[\"wdsp\"].cast(DoubleType()))\n","  df_weather = df_weather[['rain', 'temp', 'sun', 'wdsp', 'date', 'hour']]\n","  df = df.join(df_weather, on=['hour', 'date'], how='inner')\n","  return df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Integration of weather data - Task 3","showTitle":true,"inputWidgets":{},"nuid":"876af64f-655c-4f05-b39b-a73582125c93"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"source":["### Integration of twitter data - Task 3"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["def integrate_events_data(df):\n","  # df is stream\n","  df_events = spark.read.csv(\"/FileStore/tables/event_tweet_update1.csv\", header = True)\n","  df_events = df_events.withColumnRenamed('date', 'DateTime')\n","  df_events = df_events.withColumn(\"date\", F.split(col(\"DateTime\"), \" \").getItem(0)).withColumn(\"hour\", F.split(col(\"DateTime\"), \" \").getItem(1))\n","  df_events = df_events.withColumn('date', F.to_date(df_events.date, 'yyyy-MM-dd'))\n","  df_events = df_events.withColumn('hour', df_events.hour[0:2].cast(IntegerType()))\n","  df_events = df_events.withColumn(\"event\", df_events[\"event\"].cast(DoubleType()))\n","  df_events = df_events.drop('DateTime')\n","  df = df.join(df_events, on=['hour', 'date'], how='inner')\n","  return df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Integration of twitter data - Task 3","showTitle":true,"inputWidgets":{},"nuid":"31ec67bf-25dc-4b33-93cd-dd991b6d7cbc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"source":["### Hash tables for stream preprocess for prediction "],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["def create_hash_tables_for_stream_prediction():\n","  from pyspark.sql.functions import monotonically_increasing_id \n","  areaId_count = trainingData.groupBy('areaId1').count()\n","  areaId_count = areaId_count.sort(desc(\"count\"))\n","  areaId_count = areaId_count.withColumn('index', row_number().over(Window.orderBy(monotonically_increasing_id())) - 1)\n","  areaId_count.write.format('csv').option('header',True).mode('overwrite').option('sep','|').save('/areaId1_hash_new1.csv')\n","  \n","  lineId_count = trainingData.groupBy('lineId').count()\n","  lineId_count = lineId_count.sort(desc(\"count\"))\n","  lineId_count = lineId_count.withColumn('index', row_number().over(Window.orderBy(monotonically_increasing_id())) - 1)\n","  lineId_count.write.format('csv').option('header',True).mode('overwrite').option('sep','|').save('/lineId_hash.csv')\n","  \n","  journeyPatternId_count = trainingData.groupBy('journeyPatternId').count()\n","  journeyPatternId_count = journeyPatternId_count.sort(desc(\"count\"))\n","  journeyPatternId_count = journeyPatternId_count.withColumn('index', row_number().over(Window.orderBy(monotonically_increasing_id())) - 1)\n","  journeyPatternId_count.write.format('csv').option('header',True).mode('overwrite').option('sep','|').save('/journeyPatternId_hash.csv')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Hash tables for stream preprocess for prediction","showTitle":true,"inputWidgets":{},"nuid":"3ff26eab-dd01-45db-9ab6-7ae54822ed45"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"source":["### Read stream data from Kafka - Final Task"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["kafka_server = '10.0.0.30:9091'\n"," \n","# Subscribe to a pattern\n","kafka_raw_df = spark \\\n","  .readStream \\\n","  .format(\"kafka\") \\\n","  .option(\"kafka.bootstrap.servers\", kafka_server) \\\n","  .option(\"subscribePattern\", \"vehicleId_.*\") \\\n","  .option(\"startingOffsets\", \"earliest\") \\\n","  .load()\n"," \n","kafka_value_df = kafka_raw_df.selectExpr(\"CAST(value AS STRING)\")\n"," \n","schema = pickle.load(open(\"/dbfs/mnt/schema.pkl\", \"rb\"))\n"," \n","kafka_df = kafka_value_df \\\n","           .select(F.from_json(F.col(\"value\"), schema=schema).alias('json')) \\\n","           .select(\"json.*\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Read stream data from Kafka - Final Task","showTitle":true,"inputWidgets":{},"nuid":"3207ae7a-e29f-40ec-8ead-db9f77aa90b8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"source":["### Definitions for regression "],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["################################# Defenition of the columns for the regression #################################\n","catcols = {'areaId1' : 'areaId1_hash_new1.csv' ,'lineId' : 'lineId_hash.csv' , 'journeyPatternId' : 'journeyPatternId_hash.csv'}\n","num_cols = ['vehicleSpeed', 'delay', 'season','rushHours','hour' ,'congestion', 'rain', 'temp', 'sun', 'wdsp']\n","labelCol = 'event'\n","classes = [0, 1]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Definitions for regression","showTitle":true,"inputWidgets":{},"nuid":"e421964f-418b-4b18-b924-7489b93663b8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"source":["### Train the model on static historical data"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["# Task 1\n","union_df = preprocess_data(df)\n","# Task 2\n","union_df_filter_uncertain = filter_uncertain_data(union_df)\n","# droping uncertain data\n","df_no_null = union_df_filter_uncertain.na.drop()\n","# Task 3\n","df_joined_weather = integrate_weather_data(df_no_null)\n","df_joined = integrate_events_data(df_joined_weather)\n","# split the data for predition - the split is not random due to the fact that the test data is a real-time data (time biased)\n","trainingData_for_validation = df_joined.where(df_joined.date < lit('2018-05-01'))\n","data = prepare_columns_for_regression(trainingData_for_validation,catcols, num_cols,labelCol)\n","model = train(data, \"/lr_model_no_uncertain_for_validation.model\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Train the model on static historical data","showTitle":true,"inputWidgets":{},"nuid":"3984323a-b504-4a1f-84f1-88605d2a581e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"source":["### Evaluate the model on static historical data which simulates the test data - 2 new months  "],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["validationData = df_joined.where(df_joined.date >= lit('2018-05-01'))\n","validationData = validationData.where(validationData.date <= lit('2018-07-01'))\n","validationData_for_regression =  prepare_columns_for_regression(validationData,catcols, num_cols,labelCol)\n","validation_model = LogisticRegressionModel.load(\"/lr_model_no_uncertain_for_validation.model\")\n","validation_predictions = validation_model.transform(validationData_for_regression)\n","evaluation(validation_predictions, classes, \"no uncertain\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Evaluate the model on static historical data which simulates the test data - 2 new months ","showTitle":true,"inputWidgets":{},"nuid":"09ab37f5-5a6a-4cd1-a04c-a8e2c2f2fd45"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"source":["### Train the model on all the trainData (without splitting for validation)"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["trainingData = df_joined.where(df_joined.date < lit('2018-07-01'))\n","data = prepare_columns_for_regression(trainingData,catcols, num_cols,labelCol)\n","model = train(data, \"/lr_model_no_uncertain_V3.model\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Train the model on all the trainData (without splitting for validation)","showTitle":true,"inputWidgets":{},"nuid":"940d1a09-a399-4912-a7c3-946031015ca6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"source":["### Executing all tasks on the stream data"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["# Task 1\n","streaming_union_df = preprocess_data(kafka_df)\n","# Task 2\n","streaming_union_df_filter_uncertain = filter_uncertain_data(streaming_union_df)\n","streaming_df_no_null = streaming_union_df_filter_uncertain.na.drop()\n","# Task 3\n","streaming_df_joined_weather = integrate_weather_data(streaming_df_no_null)\n","streaming_df_joined = integrate_events_data(streaming_df_joined_weather)\n","\n","# Write to Elastic - drop irrelevant columns\n","streaming_df_to_elastic = streaming_df_joined.drop('areaId', 'areaId2', 'areaId3', 'atStop', 'gridId', 'ellapsedTime', 'justLeftStop', 'justStopped', '_c0')\n","streaming_df_to_elastic = streaming_df_to_elastic.withColumn('congestion', streaming_df_to_elastic.congestion.cast(BooleanType()))\n","streaming_df_to_elastic = streaming_df_to_elastic.withColumn('event', streaming_df_to_elastic.event.cast(BooleanType()))\n","# Write raw data:\n","write_stream(streaming_df_to_elastic, 'dublin_bus_with_weather_and_tweets_stream',SETTINGS, \"/tmp/Ron_Eden/Stream/\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Executing all tasks on the stream data","showTitle":true,"inputWidgets":{},"nuid":"4556129a-8e5b-40b4-9cf0-29763bcf4146"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"source":["### Predict on streaming data - last 3 month"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["streaming_test_data = streaming_df_joined.filter(streaming_df_joined.date > lit('2018-07-01')) #save last three months of the data for testing\n","test_data = prepare_columns_for_regression(streaming_test_data, catcols, num_cols, labelCol)\n","test_model = LogisticRegressionModel.load(\"/lr_model_no_uncertain_V3.model\")\n","no_null_predictions = test_model.transform(test_data)\n","# write predictions to elastic\n","write_stream(no_null_predictions[['date','hour','lineId','prediction','journeyPatternId','areaId1']], 'predictions',SETTINGS2, \"/tmp/eden/Stream4/\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Predict on streaming data - last 3 month","showTitle":true,"inputWidgets":{},"nuid":"bdcca46b-9736-4bae-b200-9446689a1087"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"source":["### Transform external data and upload to Elastic Search"],"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"865893a1-9a84-47c7-9038-112746854253"}}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Task 1\n","streaming_df_external = preprocess_data(df_external_data)\n","# Task 2\n","streaming_external_df_filter_uncertain = filter_uncertain_data(streaming_df_external)\n","streaming_external_df_no_null = streaming_external_df_filter_uncertain.na.drop()\n","# Task 3\n","streaming_external_df_joined_weather = integrate_weather_data(streaming_external_df_no_null)\n","streaming_external_df_joined = integrate_events_data(streaming_external_df_joined_weather, start_date=, end_date=)\n","\n","# Write to Elastic - drop irrelevant columns\n","streaming_df_to_elastic = streaming_external_df_joined.drop('areaId', 'areaId2', 'areaId3', 'atStop', 'gridId', 'ellapsedTime', 'justLeftStop', 'justStopped', '_c0')\n","streaming_df_to_elastic = streaming_external_df_joined.withColumn('congestion', streaming_df_to_elastic.congestion.cast(BooleanType()))\n","streaming_df_to_elastic = streaming_df_to_elastic.withColumn('event', streaming_df_to_elastic.event.cast(BooleanType()))\n","# Write raw data:\n","write_stream(streaming_df_to_elastic, 'dublin_bus_with_weather_and_tweets_stream',SETTINGS, \"/tmp/Ron_Eden/Stream/\")"]},{"source":["### Predict on the external new data"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["streaming_external_test_data = streaming_external_df_joined.filter(streaming_external_df_joined.date > lit('2018-07-01')) #save last three months of the data for testing\n","external_test_data = prepare_columns_for_regression(streaming_external_test_data, catcols, num_cols, labelCol)\n","test_model = LogisticRegressionModel.load(\"/lr_model_no_uncertain_V3.model\")\n","no_null_predictions = test_model.transform(external_test_data)\n","# write predictions to elastic\n","write_stream(no_null_predictions[['date','hour','lineId','prediction','journeyPatternId','areaId1']], 'predictions',SETTINGS2, \"/tmp/eden/Stream4/\")"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Warmup","dashboards":[],"language":"python","widgets":{},"notebookOrigID":2097931246198902}},"nbformat":4,"nbformat_minor":0}